from sklearn import datasets 
from sklearn.model_selection import train_test_split 
from sklearn import metrics 
import pandas as pd 
import matplotlib.pyplot as plt 
import  numpy as np 
iris = datasets.load_iris() 
X = iris.data 
y = iris.target 
from sklearn.ensemble import GradientBoostingClassifier 
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import RandomizedSearchCV 
from sklearn.model_selection import RepeatedStratifiedKFold 
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, \ 
f1_score, roc_auc_score, roc_curve, precision_score, recall_score 
from IPython.display import display 
import matplotlib.pyplot as plt 
import matplotlib.pyplot as plt 
plt.rcParams['figure.figsize'] = [10,6] 
import warnings  
warnings.filterwarnings('ignore') 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) 
std = StandardScaler() 
print('\033[1mStandardardization on Training set'.center(100)) 
X_train_std = std.fit_transform(X_train) 
X_train_std = pd.DataFrame(X_train_std) 
display(X_train_std.describe()) 
print('\n','\033[1mStandardardization on Testing set'.center(100)) 
X_test_std = std.transform(X_test) 
X_test_std = pd.DataFrame(X_test_std) 
display(X_test_std.describe()) 
GB_model = GradientBoostingClassifier() 
GB_model.fit(X_train_std,y_train) 
param_dist = { 
    "n_estimators":[5,20,100,500], 
    "max_depth":[1,3,5,7,9], 
    "learning_rate":[0.01,0.1,1,10,100]} 
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) 
RCV = RandomizedSearchCV(GB_model, param_dist, n_iter=50, scoring='accuracy', n_jobs=-1, cv=5, random_state=1) 
Evaluation_Results = pd.DataFrame(np.zeros((8,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score']) 
Evaluation_Results.index=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Na√Øve Bayes Classifier (NB)', 
                         'Support Vector Machine (SVM)','K Nearest Neighbours (KNN)', 'Gradient Boosting (GB)','Extreme Gradient Boosting (XGB)'] 
Evaluation_Results 
def Classification_Summary(pred,pred_prob,i): 
    Evaluation_Results.iloc[i]['Accuracy']=round(accuracy_score(y_test, pred),3)*100    
    Evaluation_Results.iloc[i]['Precision']=round(precision_score(y_test, pred, average='weighted'),3)*100 # 
    Evaluation_Results.iloc[i]['Recall']=round(recall_score(y_test, pred, average='weighted'),3)*100 # 
    Evaluation_Results.iloc[i]['F1-score']=round(f1_score(y_test, pred, average='weighted'),3)*100 # 
    Evaluation_Results.iloc[i]['AUC-ROC score']=round(roc_auc_score(y_test, pred_prob, multi_class='ovr'),3)*100 #[:, 1] 
    print('{}{}\033[1m Evaluating {} \033[0m{}{}\n'.format('<'*3,'-'*35,Evaluation_Results.index[i], '-'*35,'>'*3)) 
    print('Accuracy = {}%'.format(round(accuracy_score(y_test, pred),3)*100)) 
    print('F1 Score = {}%'.format(round(f1_score(y_test, pred, average='weighted'),3)*100)) # 
    print('\n \033[1mConfusiton Matrix:\033[0m\n',confusion_matrix(y_test, pred)) 
    print('\n\033[1mClassification Report:\033[0m\n',classification_report(y_test, pred)) 
     
 
 
 
GB = RCV.fit(X_train_std, y_train).best_estimator_ 
pred = GB.predict(X_test_std) 
pred_prob = GB.predict_proba(X_test_std) 
Classification_Summary(pred,pred_prob,6) 
 
 
from sklearn.metrics  import RocCurveDisplay 
RocCurveDisplay.from_predictions(y_test,pred,pos_label=1) 


